# -*- coding: utf-8 -*-
"""Tokenizacao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FJWK4QxfpKF1zCUC6WcyTgACCY646doR

Este notebook tem o objetivo de apresentar as t√©cnicas de tokeniza√ß√£o, atividade de quebrar o texto em peda√ßos com significado sem√¢ntico.

Ser√£o exploradas as seguintes t√©cnicas e ferramentas:

* m√©todo ‚Äúsplit‚Äù;
* express√µes regulares
* Word Tokenizer, Sentence Tokenizer
* Treebank Tokenizer
* Tweet Tokenizer

Para explora√ß√£o das t√©cnicas, consideraremos a seguinte senten√ßa extra√≠da do resumo do romance Dom Quixote de La Mancha, de Miguel de Cervantes, obtido em https://www.culturagenial.com/livro-dom-quixote-de-miguel-de-cervantes/ :

"A obra narra as aventuras e desventuras de Dom Quixote, um homem de meia idade que resolveu se tornar cavaleiro andante depois de ler muitos romances de cavalaria. Providenciando cavalo e armadura, resolve lutar para provar seu amor por Dulcineia de Toboso, uma mulher imagin√°ria. Consegue tamb√©m um escudeiro, Sancho Pan√ßa, que resolve acompanh√°-lo, acreditando que ser√° recompensado."
"""

texto = ("A obra narra as aventuras e desventuras de D. Quixote, um homem de "
          "meia idade que resolveu se tornar cavaleiro andante depois de ler muitos romances "
          "de cavalaria. Providenciando cavalo e armadura, resolve lutar para provar seu amor "
          "por Dulcineia de Toboso, uma mulher imagin√°ria. Consegue tamb√©m um escudeiro, "
          "Sancho Pan√ßa, que resolve acompanh√°-lo, acreditando que ser√° recompensado...")

"""#### Utilizando o m√©todo *split*"""

tokens = texto.split()

tokens

"""Um dos problemas dessa abordagem √© a pontua√ß√£o, que fica vinculada a um token, prejudicando a informa√ß√£o dele.

#### Utilizando o express√µes regulares
"""

# padr√£o 1
import re
regex = r'\w+'
re.findall(regex, texto)

"""Nessa solu√ß√£o a pontua√ß√£o √© descartada. Como seria a express√£o se o interesse for tokenizar os sinais de pontua√ß√£o tamb√©m?"""

# padr√£o 2
regex = r'\w+|\S'
re.findall(regex, texto)

"""#### Word Tokenizer & Sentence Tokenizer
Recursos padr√£o da NLTK para tokeniza√ß√£o de palavras ou senten√ßas. 
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('punkt')

sent_tokenize(texto)

word_tokenize(texto)

"""#### TreeBank Tokenizer √© baseado em express√µes regulares. Tem mais diferencia√ß√£o em textos em ingl√™s devido ao bom tratamento nas situa√ß√µes de palavras em contra√ß√£o. Ex: I'm ; doesn't"""

from nltk.tokenize import TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
tokenizer.tokenize(texto)

"""#### Tweet Tokenizer - a manipula√ß√£o de tweets √© sempre trabalhosa pela quantidade de remo√ß√µes de trechos sem valor que devem ser feitas no texto. Este tokenizador √© especializado nisso."""

tweet1='RT @Flamengo: Mais um dia de chuva para os cariocas. Fala, @gabigol! ü•∂üåßÔ∏è #VamosFlamengo https://t.co/MXQhQMhSFT'
tweet2='RT @ESPNBrasil: RT quem quer ver dancinha do Vini Jr contra o Atl√©tico de Madrid! #BailaViniJr #LaLigaNaESPN https://t.co/P84meYNjVQ'
tweet3='RT @4evercrf__: outubro ou vai ser o m√™s mais feliz da minha vida ou o pior, est√° nas m√£os do Flamengo.'

# Utilizando o word_tokenizer
word_tokenize(tweet1)

from nltk.tokenize import TweetTokenizer
tokenizer=TweetTokenizer()
tokenizer.tokenize(tweet1)

"""O TweetTokenizer reconhece quest√µes como men√ß√£o a usu√°rios e hashtags."""

# utilizando o par√¢metro 'strip_handles' retira as men√ß√µes no texto 
tokenizer = TweetTokenizer(strip_handles=True)
tokenizer.tokenize(tweet1)

"""## Tokeniza√ß√£o de tamanhos maiores que 1
Os tokens com apenas 1 palavra s√£o chamados de unigramas. Mas ocorrem situa√ß√µes em que √© interessante tratar as palavras vizinhas com o intuito de capturar toda a informa√ß√£o.

Exemplo: 'S√£o Paulo' -> ['S√£o', 'Paulo']

Dessa forma, quando for interessante os tokens podem ser agrupados em bigramas ou trigramas, de modo geral n-gramas.
"""

# Cria√ß√£o de bigramas
from nltk.util import ngrams
tokens = re.findall(regex, texto)
bigrams = list(ngrams(tokens, 2))
[" ".join(token) for token in bigrams]

# Cria√ß√£o de trigramas
from nltk.util import ngrams
trigrams = list(ngrams(tokens, 3))
[" ".join(token) for token in trigrams]