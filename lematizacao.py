# -*- coding: utf-8 -*-
"""Lematizacao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QxCuGhHYYlMCOGQHcVD0H5H82lvnMiFP

### Explorando operações de Lematização (lemmatization)

Utilizando a NLTK
"""

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

s_portuguese = "Estamos interessados em aprofundar os estudos em NLP"
s_english = "We are putting in efforts to enhance our understanding of Lemmatization"

token_list_portuguese = s_portuguese.split()
token_list_english = s_english.split()

# Lematização da lista em portugês
print("The tokens are: ", token_list_portuguese)
lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token \
in token_list_portuguese])
print("The lemmatized output is: ", lemmatized_output)

# Lematização da lista em inglês
print("The tokens are: ", token_list_english)
lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token \
in token_list_english])
print("The lemmatized output is: ", lemmatized_output)

"""Em ambos idomas o resultado pouco agregou...

O WordNet funciona melhor se for indicada a classe sintática (POS Tagging) de cada termo como entrada.

Entretanto, o NLTK não possui um método de POS Tagging para português... Vamos nos restringir ao inglês, por hora...
"""

nltk.download('averaged_perceptron_tagger')
pos_tags = nltk.pos_tag(token_list_english)
pos_tags

from nltk.corpus import wordnet
lemmatizer.lemmatize('are', wordnet.VERB)

"""Verifique a diferença para a lematização a depender da classe sintática."""

lemmatizer.lemmatize('understanding', wordnet.NOUN)

lemmatizer.lemmatize('understanding', wordnet.VERB)

"""**Moral da história:** Para utilizar o lematizador WordNet temos que passar o parâmetro da classe sintática da palavra.

#### Conhecendo o spaCy Lemmatizer

O spaCy Lemmatizer prescinde de incluir o POS Tagging.
"""

import spacy
nlp = spacy.load("en_core_web_sm")

s_english

doc = nlp(s_english)
" ".join([token.lemma_ for token in doc])

"""Em português:"""

!spacy download pt_core_news_sm

nlp = spacy.load("pt_core_news_sm")

s_portuguese

doc = nlp(s_portuguese)
" ".join([token.lemma_ for token in doc])

for item in nlp('é'):
  print(item.lemma_)

"""Teste a lematização das sentenças: 'São Paulo', 'são paulo', 'SÃO PAULOS', 'São', 'Paulo': """

