# -*- coding: utf-8 -*-
"""HelloWorld_NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lSLeiQ_AZOd8hsxeVkJRqa79xflbMLe_

Este notebook tem o objetivo de apresentar de forma breve alguns recursos da biblioteca NLTK (*Natural Language Toolkit*).
Biblioteca desenvolvida por Steven Bird e Edward Loper, ambos da Universidade da Pensilvânia, muito adotada no ambiente acadêmico.

Neste [link](https://www.nltk.org/), encontra-se documentação bem detalhada.

Ela implementa as seguintes funcionalidades:

* Tokenizing
* POS Tagging
* Stemming
* Sentiment Analysis
* Topic Segmentation
* Named Entity Recognition

Para exploração dos recursos da biblioteca, consideraremos a seguinte sentença extraída do resumo do romance `Dom Quixote de La Mancha`, de Miguel de Cervantes, obtido em https://www.culturagenial.com/livro-dom-quixote-de-miguel-de-cervantes/ :

"A obra narra as aventuras e desventuras de Dom Quixote, um homem de meia idade que resolveu se tornar cavaleiro andante depois de ler muitos romances de cavalaria. Providenciando cavalo e armadura, resolve lutar para provar seu amor por Dulcineia de Toboso, uma mulher imaginária. Consegue também um escudeiro, Sancho Pança, que resolve acompanhá-lo, acreditando que será recompensado."
"""

import nltk

# Download dos módulos a serem utilizados
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

sentenca = ("A obra narra as aventuras e desventuras de Dom Quixote, um homem de "
          "meia idade que resolveu se tornar cavaleiro andante depois de ler muitos romances "
          "de cavalaria. Providenciando cavalo e armadura, resolve lutar para provar seu amor "
          "por Dulcineia de Toboso, uma mulher imaginária. Consegue também um escudeiro, "
          "Sancho Pança, que resolve acompanhá-lo, acreditando que será recompensado.")

# Tokenizacao por sentenças
from nltk.tokenize import sent_tokenize
tokenized_text=sent_tokenize(sentenca)
print(tokenized_text)

# Quantidade de sentenças separadas
len(tokenized_text)

"""Será que o único critério é a existência do ponto para a limitação entre as sentenças??

Teste com a sentença: `Ele viajou a pé por São Paulo a convite da entidade D. Cabral e faturou R$ 2.000,00.`
"""



# Tokenizacao por palavras
from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(sentenca)
print(tokenized_word)

# Distribuição de frequencia dos tokens
from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word)
print(fdist)

# Para verificar a lista
fdist.elements

# Mostrar os 5 tokens mais frequentes
fdist.most_common(5)

# Remoção de stopwords

from nltk.corpus import stopwords
# carga da relação básica de stopwords 
stop_words=set(stopwords.words("portuguese"))
print(stop_words)

filtered_sent=[]
for w in tokenized_word:
    if w not in stop_words:
        filtered_sent.append(w)
print("Relação original:",tokenized_word)
print("Sem stopwords:",filtered_sent)

"""A filtragem foi efetiva?? O que poderia ser feito para melhorar o resultado?"""

# Stemming utilizando o PorterStermer
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

ps = PorterStemmer()

stemmed_words=[]
for w in filtered_sent:
    stemmed_words.append(ps.stem(w))

print("Relação original:",filtered_sent)
print("Relação processada:",stemmed_words)

# Stemming utilizando o Snowball
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

stemmer = SnowballStemmer("portuguese")

stemmed_words=[]
for w in filtered_sent:
    stemmed_words.append(stemmer.stem(w))

print("Relação original:",filtered_sent)
print("Relação processada:",stemmed_words)

"""Qual a diferença entre os resultados???"""

# Lemmatization
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()

from nltk.stem.porter import PorterStemmer
stem = PorterStemmer()

word = "resolve"
print("Lemmatized Word:",lem.lemmatize(word,"v"))
print("Stemmed Word:",stem.stem(word))

"""O WordNet Lemmatizer, disponível no NLTK, necessita da informação sobre a classe sintática do termo. 

Não tem suporte para o português. Dessa forma, deve-se buscar outra biblioteca para a lematização de conteúdo em português.
"""

# POS Tagging
tokens=nltk.word_tokenize(sentenca)
print(tokens)
nltk.pos_tag(tokens)

"""Conforme já estudado, a tarefa de POS Tagging é identificação das classes sintáticas dos tokens. Dito isso, faz sentido a classificação obtida??"""