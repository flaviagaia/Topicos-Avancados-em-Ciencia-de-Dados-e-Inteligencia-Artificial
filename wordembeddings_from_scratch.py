# -*- coding: utf-8 -*-
"""WordEmbeddings_from_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18WDH4ZvO2UNsxiyNovM3pjOWUP01EcMB

## Construir uma estrutura de WordEmbeddings do zero

A estrutura de Word Embeddings é muito atrelada ao contexto. Por vezes, é preciso criar uma estrutura com base no corpus que está sendo utilizado.

### Inicialmente será criado um modelo baseado no Word2vec

Para simplificação, será considearado um corpus com 6 sentenças. Extraídas do dataset que possui avaliações de clientes das lojas americanas.
"""

import pandas as pd

df_full = pd.read_csv('https://raw.githubusercontent.com/alexvaroz/data_science_alem_do_basico/master/americanas_analise_sentimento_preparado.csv')

df_full.head()

positivos = df_full[df_full.sentiment=='positive'].sample(3)
negativos = df_full[df_full.sentiment=='negative'].sample(3)
sample = pd.concat([positivos, negativos], ignore_index=True )

sample.review_text.values

sentences = ['gostaria de saber se é a versão global pois tenho medo de não vir',
             'Funciona perfeitamente e atende a todas as necessidades que se \
             propõe atender Apenas senti falta do pente para aparar sobrancelhas\
              que nos modelos anteriores a Phillips incluía no pacote',
             'Excelente produto imagem e som melhor que cinema sistema \
             operacional nota 10',
             'Estou tentando contado com a americanas.com desde quinta\
             e sem sucesso O produto veio errado E NÃO EXISTE SUPORTE DA \
             AMERICANAS Por favor fico no aguardo de como proceder \
             Pois veio uma máquina que custa a metade do valor que paguei pelo \
             site de vocês',
             'Ele não atende as expectativas porque ele é muito pequeno e não \
             tem como fazer nada com ele ele simplesmente É para decoração e se \
             você quer um para usar eu não recomendo',
             'Produto ruim e com fone de ouvido defeituoso Celular trava muito \
             e a resolução e muito ruim']

from gensim.models import Word2Vec

# Tokenizando de forma básica os documentos, visto que já tinha sido extraída a pontuação
sentences = [sentence.lower().split() for sentence in sentences]

sentences

"""Os principais parâmetros a serem considerados para a criação do modelo são:
* size: número de dimensões do vetor de palavras que será gerado. O valor padrão é 100.
* window: vai definir quantas palavras deverão ser consideradas na janela deslizante para o treinamento do modelo. Valor padrão é 5.
* min_count: especifica a quantidade mínima de ocorrência para uma palavra constar no vocabulário. Valor padrão é 5.
* sg: especifica o algoritmo de treinamento do modelo (0: cbow ou 1:skipgram). Valor padrão é 0.

Mais informações sobre esses parâmetros podem ser encontrados em: 
https://radimrehurek.com/gensim/models/word2vec.html
"""

# Para criar o modelo
model = Word2Vec(sentences, min_count=1)

len(model.wv.vocab)

model.vector_size

model.wv['produto']

# Tamanho do de cada vetor de palavra
model.vector_size

# Tamanho do vocabulário
len(model.wv.vocab)

model.most_similar('produto')

# Restringindo o vocabulário
model_2 = Word2Vec(sentences, min_count=2)

len(model_2.wv.vocab)

model_2.wv['produto']

model_2.most_similar('produto')

# Para salvar o modelo
model.save('model.bin')

# Para carregar um modelo previamente salvo
model_loaded=Word2Vec.load('/content/model.bin')

model_loaded.vector_size

"""## Visualizar a relação entre as palavras do modelo"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

plt.subplots(figsize=(12,8))
X = model[model.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# create a scatter plot of the projection
plt.scatter(result[:, 0], result[:, 1])
words = list(model.wv.vocab)
for i, word in enumerate(words):
  plt.annotate(word, xy=(result[i, 0], result[i, 1]))
plt.show()

model.most_similar('celular')

"""## Exercício
Ampliar a análise para 200 registros.
"""

!spacy download pt_core_news_sm -q

import spacy

nlp = spacy.load('pt_core_news_sm')

def sentence_tokenizer(sentence):
  return [token.lemma_ for token in nlp(sentence.lower()) 
              if (token.is_alpha & ~token.is_stop)]

sample_sentences = df_full.review_text.sample(200)

tokenized_sentences = [sentence_tokenizer(sentence) for sentence in sample_sentences.values 
                       if pd.notnull(sentence)]

len(tokenized_sentences)

model = Word2Vec(tokenized_sentences, min_count=1, size=50)

model.most_similar('compre')

plt.subplots(figsize=(12,8))
X = model[model.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# create a scatter plot of the projection
plt.scatter(result[:, 0], result[:, 1])
words = list(model.wv.vocab)
for i, word in enumerate(words):
  plt.annotate(word, xy=(result[i, 0], result[i, 1]))
plt.show()

"""## Para classificar um texto??

Para utilizar essa estrutura para representar um texto, temos 2 abordagens:

* utilizar a média dos valores do array de cada palavra da sentença;
* utilizar a média ponderando com o fator tfidf.

Uma outra opção é utilizar o algoritmo Doc2Vec, que se mostra muito mais prático.

### Utilizando o Doc2Vec
"""

from gensim.models.doc2vec import Doc2Vec, TaggedDocument

sample

tokenized_sentences = [sentence_tokenizer(sentence) for sentence in sample.review_text.values 
                       if pd.notnull(sentence)]

# Corpus de treinamento
tokenized_sentences

documents = [TaggedDocument(doc, [i]) for i, doc in \
enumerate(tokenized_sentences)]
documents

model = Doc2Vec(documents, vector_size=20, min_count=1)
model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)

model.vector_size

len(model.docvecs)

model.docvecs[0]

list(model.docvecs[0])

"""Pronto!! Temos o nosso corpus vetorizado. Agora é partir para os algoritmos de ML."""

text_representation = [list(model.docvecs[i]) for i in  range(0, len(model.docvecs))]

text_representation

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()

rf.fit(text_representation, sample.sentiment)

rf.predict([text_representation[0]])



# Para acrescentar mais um documento ao modelo
novo_array = ['não', 'gostei', 'do', 'produto']
vector = model.infer_vector(novo_array)
print(vector)

model.most_similar('bom')

texto_teste = 'Achei o produto muito dificil!!'
rf.predict([list(model.infer_vector(sentence_tokenizer(texto_teste)))])



"""## Utilizando a média dos valores do Word2Vec

$$AWV(D) = \frac{\sum^{n}_{1}\,wv(w)}{n}$$
"""

# criar um outro modelo com menos dimensões

model = Word2Vec(sentences, min_count=1, size=5)

import numpy as np

def average_word_vectors(words, model):
  feature_vector = np.zeros((model.vector_size,),dtype="float64")
  nwords = 0
  for word in words:
    if word in model.wv.vocab:
      nwords = nwords + 1
      feature_vector = np.add(feature_vector, model[word])
  if nwords:
    feature_vector = np.divide(feature_vector, nwords)
  return feature_vector

# generalize above function for a corpus of documents
def averaged_word_vectorizer(corpus, model):
  features = [average_word_vectors(tokenized_sentence, model) for 
              tokenized_sentence in corpus]
  return np.array(features)

avg_word_vec_features = averaged_word_vectorizer(corpus=sentences, model=model)

# As sentenças foram transformadas em cada vetor abaixo
avg_word_vec_features

"""#### Utilizando o TFIDF

$$ TWA(D) = \frac{\sum^{n}_{1}\,wv(w) \times tfidf(w)}{n}$$
"""

sentences_to_tfidf = [' '.join(sentence) for sentence in sentences]

sentences_to_tfidf

# Primeiro passo, obter a matriz TFIDF

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(sentences_to_tfidf)
tfidf_vocabulary = vectorizer.get_feature_names()

'produto' in tfidf_vocabulary

tfidf_matrix[0].toarray()[0,64]

tfidf_matrix.shape

tfidf_vocabulary[9]

def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model):
  word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.index(word)] 
                 if word in tfidf_vocabulary else 0 for word in words]
  word_tfidf_map = {word:tfidf_val for word, tfidf_val in 
                    zip(words, word_tfidfs)}
  feature_vector = np.zeros((model.vector_size,),dtype="float64")
  vocabulary = set(model.wv.vocab)
  wts = 0.
  for word in words:
    if word in vocabulary:
      word_vector = model[word]
      weighted_word_vector = word_tfidf_map[word] * word_vector
      wts = wts + word_tfidf_map[word]
      feature_vector = np.add(feature_vector, weighted_word_vector)
  if wts:
    feature_vector = np.divide(feature_vector, wts)
  return feature_vector


def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, 
                                            tfidf_vocabulary, model):
  docs_tfidfs = [(doc, doc_tfidf) for doc, doc_tfidf in zip(
      corpus, tfidf_vectors)]
  features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf.toarray(), 
                                         tfidf_vocabulary, model) for 
              tokenized_sentence, tfidf in docs_tfidfs]
  return np.array(features)

result = tfidf_weighted_averaged_word_vectorizer(corpus=sentences, 
                                        tfidf_vectors=tfidf_matrix, 
                                        tfidf_vocabulary=tfidf_vocabulary,
                                        model=model)

"""# Exercício:
Implementar o modelo de classificação usando Doc2Vec no dataset de notícias da folha, trabalhado na aula passada. 
"""