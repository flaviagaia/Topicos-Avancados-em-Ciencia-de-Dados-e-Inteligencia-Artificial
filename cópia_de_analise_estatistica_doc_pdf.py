# -*- coding: utf-8 -*-
"""Cópia de Analise_Estatistica_Doc_PDF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pj_maFD8PGpAH-k-H52NYq1GaxXweer5

### Extração de textos de PDF

Na construção de aplicações que lidam com problemas de Processamento de Linguagem Natural, é comum a necessidade de consumir informações presentes em arquivos PDF. Neste notebook será apresentado um exemplo de utilização de uma biblioteca python recomendada para isso: [PDF Plumber](https://github.com/jsvine/pdfplumber).

Importante destacar que existem várias outras bibliotecas dedicadas a essa tarefa, inclusive, este [post](https://towardsdatascience.com/how-to-extract-text-from-pdf-245482a96de7) apresenta uma comparação entre algumas.
"""

# instalação
!pip install pdfplumber -q

import pdfplumber
import os
import pandas as pd
from itertools import islice

documento_analisado='/content/drive/MyDrive/TopicosNLP-02_2022/Notebooks/acordaos_tcu/acordao_tcu_3.pdf'

pdf = pdfplumber.open(documento_analisado)

pdf

"""### Páginas do documento
Para acessar as páginas existentes no arquivo, utilizamos a função `pages`.

Ela retorna uma lista com as páginas do arquivo.
"""

len(pdf.pages)

page = pdf.pages[0]
page.page_number

pdf.metadata

text = page.extract_text()
print(text)

"""Pegando todas as páginas do documento:"""

inteiro_teor = ''
for page in pdf.pages:
  inteiro_teor=inteiro_teor + page.extract_text()
  inteiro_teor=inteiro_teor + ' '

print(inteiro_teor)

import re

assinatura_rodape=r'\s*Para verificar as assinaturas, acesse www\.tcu\.gov\.br/autenticidade.+\d\.'
cabecalho=r'\s*TRIBUNAL DE CONTAS DA UNIÃO\s*TC \d{3}\.\d{3}/\d{4}-\d'

# Limpando
inteiro_teor_limpo=re.sub(assinatura_rodape, '', inteiro_teor)
inteiro_teor_limpo=re.sub(cabecalho, '', inteiro_teor_limpo)
print(inteiro_teor_limpo)

# Separar em partes RELATÓRIO, VOTO, ACORDAO
regex_partes=r'\s*(RELATÓRIO\s*.*É o relatório\.).*(VOTO\s*.*\s*)(ACÓRDÃO\s*.*Procurador\w?-Geral)'
print(re.search(regex_partes, inteiro_teor_limpo, re.DOTALL).groups()[0])

content={}
content['RELATORIO']=re.search(regex_partes, inteiro_teor_limpo, re.DOTALL).groups()[0]
content['VOTO']=re.search(regex_partes, inteiro_teor_limpo, re.DOTALL).groups()[1]
content['ACORDAO']=re.search(regex_partes, inteiro_teor_limpo, re.DOTALL).groups()[2]

"""Criar umas estatísticas sobre o texto??

* Quais palavras mais citadas?
* Quais bigramas mais citados?
* Quais tri-gramas mais citados?
* Nuvem de palavras.

Para rodar estatísticas, nesta etapa do curso, vamos definir as seguintes regras:
* Não são considerados sinais de pontuação;
* São transformados todos para *lower case*;
* Não serão considerados os números pois o interesse é no vocabulário, e não no conteúdo;
* Não serão realizados os processos de setmização ou lematização por enquanto.
"""

# Transformação em tudo lowercase
content['RELATORIO']=content['RELATORIO'].lower()

# Elminiação dos números
content['RELATORIO']=re.sub(r'\d','', content['RELATORIO'] )

# tokenizacao
regex_token='\w+'
tokens=re.findall( regex_token,content['RELATORIO'])

# remoção stopwords
import nltk
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('portuguese')
tokens_limpos=[]
for item in tokens:
  if item not in stopwords:
    tokens_limpos.append(item)

# 30 mais comuns
from collections import Counter
stats_tokens_limpos = Counter(tokens_limpos)

tokens_limpos

sorted = stats_tokens_limpos.most_common(30)
sorted

words_tokens = [word[0] for word in sorted[0:29]]
freq_tokens = [word[1] for word in sorted[0:29]]

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(words_tokens, freq_tokens)

ax.set(xlabel='Palavras', ylabel='Frequência',
       title='Distribuição da frequência dos tokens')
plt.xticks(rotation=90)
ax.grid()

plt.show()

# bigramas
bigrams= [*map(' '.join, zip(tokens_limpos, islice(tokens_limpos, 2, None)))]

bigrams

stats_bigrams = Counter(bigrams)
stats_bigrams = stats_bigrams.most_common(30)

words_tokens = [word[0] for word in stats_bigrams[0:29]]
freq_tokens = [word[1] for word in stats_bigrams[0:29]]

stats_bigrams

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(words_tokens, freq_tokens)

ax.set(xlabel='Palavras', ylabel='Frequência',
       title='Distribuição da frequência dos tokens')
plt.xticks(rotation=90)
ax.grid()

plt.show()

# nuvem de palavras
# instalar wordcloud
!pip install wordcloud -q

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

# concatenar as palavras
all_tokens = " ".join(s for s in tokens_limpos)

# concatenar as palavras
all_tokens = " ".join(s for s in bigrams)

wordcloud = WordCloud(background_color="black").generate(all_tokens)

# mostrar a imagem final
fig, ax = plt.subplots(figsize=(10,6))
ax.imshow(wordcloud, interpolation='bilinear')

plt.tight_layout()

